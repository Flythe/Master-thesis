\paragraph{What is big data?}
With the buzzword `big data' people often associate terms like size, volume, and analytics.
However, there are a lot of other data manipulation challenges that can lead to data being classified as ``big ''.
McAfee \cite{dsb1mcafee} describes big data as lots of data coming in at a high pace from many different sources, in large `volume, velocity, variety'.
Jacobs \cite{dsb5jacobs} presents it from a changing perspective of technical possibilities.
In the 1980s 100GB of data was considered big, but now the perspective has changed: what you try to do with the data makes it big or not.
Lynch \cite{dsb3lynch} stresses the problem of `lasting' (big data preservation), \ie{} how do we model and preserve the registered (sometimes unique) events.

There are wide gaps between these definitions but also similarities.
One overarching idea about big data is that it can help understand specific domains and help make decisions \cite{dsb2lohr}.
Jacobs even states that transactions and storage of data are already largely solved problems \cite{dsb5jacobs}.
This leaves decision making, modelling, and preservation as the main remaining challenges.

Big data in the corporate world mostly means management and quick reaction to real life events.
A good example is flu prediction: Google is a week faster in predicting hospital visits related to flu than the official government sources \cite{dsb8dugas, dsb1mcafee}.
McAfee~\cite{dsb1mcafee} even states that ``Data-driven decisions are better than expert-opinion decisions''.

As decisions are based on the interpretation of the data, modelling of data should reflect events in the real world.
To make the event interpretable to the machine, the event should be recorded in a structured manner.
Recording and keeping this data from events for long periods of time, such that it can be used for decision making, is the last challenge of preservation.
For example, losing data can be of significance as each event is unique and will not occur again in the same way.
There are also side effects: keeping any data (specifically medical) about persons raises many privacy challenges \cite{dsb1mcafee}.

\paragraph{Big data for \project{} research}
The \project{} dataset (\projectdata{}) consists of linked data from \IVF{}-clinics and the \PRN{}.
In the context of the \projectdata{} two of the big data factors lead to challenges: decision making and preservation.
These are mainly human related or procedural, \eg{} ethics, trust, expectancy, lack of organisational support, etc.
Modelling of data is (currently) quite straightforward, as mainly data has to be ready as input material for popular statistical software like SPSS \cite{spssSoftware} or R \cite{rSoftware}.
Introducing this model to computerised decision making may result in semantic or metadata problems, but compared to the other challenges these can be handled quite easily.

Currently researchers make decisions on many levels, \eg{} what relevant hypotheses exist, which research hypothesis to pursue, what data should be analysed, how data should be interpreted.
Many of these decisions can be supported with computerised systems.
For example, a hypothesis ``sweep'' can be executed with data mining operations, finding correlation in the data.
However, many clinical researchers hold on to generation of hypothesis based on expertise, possibly leading to missed (important) conclusions.
This might describe a trust or expectancy issue with computerised systems, or the actual value of such a data mining system was never demonstrated in practice.

On the other hand are the problems preservation poses.
Funding bodies demand more of researchers considering data-management and sharing \cite{dsb3lynch}.
These demands can even extend beyond the duration of the funding, resulting in long lasting storage issues but also providing more opportunities for reuse.
For individual research projects this can be problematic as decisions on this level should be made at a institutional control \cite{dsb3lynch}.
In this project, because assisted pregnancies are relatively rare and data gathering is a troublesome process, reuse should be encouraged to make the effort useful and significant.

Lastly, preservation and reuse of data will also throw up barriers for the data deliverers.
Right now success percentages of clinics are being published as this is required by law, however they complain that the patient mix between clinics is unfair.
Clinics want to cooperate in the \project{} but they are afraid that research outcomes will be published in a way that will reflect directly on individual clinics. 
Trust needs to be gained by all the actors involved to fully exploit the value of the \project{}.

%3 - Big data- How do your data grow?
%It also includes defining and recording appropriate metadata — such as experimental parameters and set-up — to allow for data interpretation
%This is best done when the data are captured. Indeed, descriptive metadata are often integrated within the experimental design. Description includes tracing provenance 

%1 - Big data the management revolution
%Perhaps even more important are skills in cleaning and organizing large data sets; the new kinds of data rarely come in structured formats. Visualization tools and techniques are also increasing in value.
%The best data scientists are also comfortable speaking the language of business and helping leaders reformulate their challenges in ways that big data can tackle
%The technologies are new and in some cases exotic. 
%It's too easy to mistake correlation for causation and to find misleading patterns in the data. 
%few things are more powerful for changing a decision-making culture than seeing a senior executive concede when data have disproved a hunch.
%They'll be valued not for their HiPPO-style answers but because they know what questions to ask
%When it comes to knowing which problems to tackle, of course, domain expertise remains critical
%Big data's power does not erase the need for vision or human insight.