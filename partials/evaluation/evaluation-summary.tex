%\paragraph{Summary}
\section{Summary}
\label{evaluation-summary}

As with brainstorming creating a user interface is not an exact science.
For evaluation ends design heuristics are used, and the found problems can be mapped against these.
The used list is from Nielsen \cite{designHeuristics} and contains ten famous heuristics:

\begin{enumerate}
	\item Visibility of system status;
	\item Match between system and the real world;
	\item User control and freedom;
	\item Consistency and standards;
	\item Error prevention;
	\item Recognition rather than recall;
	\item Flexibility and efficiency of use;
	\item Aesthetic and minimalist design;
	\item Help users recognise, diagnose and recover from errors;
	\item Help and documentation.
\end{enumerate}

\noindent{} Based on the system's process and the potential as a supporting factor in doing research the system got positive feedback.
One of the testers mentioned that the system as a prototype might be used for demoing purposes.
Relatively simple functions from the system can show that thought went into the workflow of data security.
Thereby fulfilling the goal of persuading data deliverers and providing more trust.

The biggest hit is the request management process.
From multiple perspectives the system can show what requests are in progress and information is available to make request `dashboards' for management purposes.
Which also brings possibilities for monitoring by data owners.

Design-wise the data view is a perfect example for the user freedom and flexibility heuristics.
There are three ways of viewing namely: raw data, graph, aggregated.
The user may switch between these views and can pick whichever they prefer for their current task.
On the other hand, the fact that the data view does not support (analogue) printing of data shows lack in flexibility.

There are things to clean up, most of the encountered problems can be related to the system status visibility.
For example, when the user is in the data dictionary the buttons on the basket do not account for this.
Which means that two out of the three buttons are completely out of context for what the user is doing.
System visibility problems are also reflected in the fact that testers tried to `start' the search by hitting enter and not noticing that the results had already updated.
Lastly, selecting data and putting it in the basket caused confusing meaning that the purpose of the basket is not well understood.

As for the heuristic `recognition rather than recall' a few issues were found as well.
The `select all' function on the basket confused the user, thinking they would select all data which was already \emph{in} the basket.
Clicking this button made the user loose all progress made resulting in a problem with recognising and recovering from errors.
Also, going from a basket selection to preparing a request was unclear as they were looking in the wrong place.
Lastly, the redirect from request overview to new message was unexpected behaviour.